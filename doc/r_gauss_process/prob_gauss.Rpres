Gaußprozesse und probabilistisches Programmieren in R
========================================================
author: Christoph Schmalhofer
date: 2019-09-17
autosize: true

Gaußprozesse sind überall
========================================================


```{r gpytorch, echo=FALSE, fig.cap="", out.width = '70%'}
knitr::include_graphics("gpytorch.png")
```

Wahrscheinlichkeitsverteilung von Funktionen

Bayes: Daten -> A-posteriori Gaußprozess


Anwendungsbeispiele
========================================================

- Bayes Optimization (Dateneffizienz)
- Zeitreihen
- Nachrichtentechnik
- Geostatistik

```{r surface, echo=FALSE, fig.cap="NCAR fields package", out.width = '70%'}
knitr::include_graphics("surface_spatialProcess_elevation.png")
```

(NCAR fields package)

Interpolation
========================================================
- Bayes Posterior versus Konfidenzintervall
```{r}
library(ggplot2)
n=10
x<-1:n
y<-x + rnorm(n)
ggplot(data=data.frame(x,y), aes(x=x,y=y)) + geom_point() + geom_smooth(method="loess", level=0.99, span=0.6)

```

Einfache Gaußprozesse
========================================================

- Weißes gaußsches Rauschen
- Lineares Modell 
- Random Walk
- Ornstein-Uhlenbeck


Weißes gaußsches Rauschen
========================================================
```{r, out.width = '70%'}
plot(rnorm(100))
```

Lineares Modell
========================================================

```{r, out.width = '70%'}
plot(1:100 + rnorm(100, sd=5))
```

Random Walk I
========================================================
```{r, out.width = '70%'}
plot(cumsum(rnorm(100)))
```

Random Walk II
========================================================

```{r, out.width = '70%'}
matplot(replicate(5, cumsum(rnorm(100))), type="l")
```

Ornstein-Uhlenbeck
=====================================================
```{r}
#https://quant.stackexchange.com/questions/1260/r-code-for-ornstein-uhlenbeck-process
ornstein_uhlenbeck <- function(T,n, x0,nu,lambda,sigma){
dw  <- rnorm(n, 0, sqrt(T/n))
dt  <- T/n
x <- c(x0)
for (i in 2:(n+1)) {
x[i]  <-  x[i-1] + lambda*(nu-x[i-1])*dt + sigma*dw[i-1]
}
return(x);
}
```

T: Dauer

n:Anzahl Werte

x0: Startwert

nu: Attraktor

lambda: Attraktionskraft

sigma: Varianz

Ornstein-Uhlenbeck (Plot)
========================================================

```{r, out.width = '70%'}
plot(ornstein_uhlenbeck(T=2,n=1000,x0=5,nu=0,lambda=2,sigma=0.5))
```


Zufallsvariablen -> zufällige Funktion
========================================================

- Wiener, Kolmogoroff in etwa 1930: 

 Wahrscheinlichkeitsverteilung über Funktionen
 
 (Kolmogorov consistency/extension theorem)
 
- Heunen, Kammar, Staton, Yang: 

  "But standard probability theory cannot support higher-order functions, that is, the category of measurable spaces is not cartesian closed"

Zufällige Funktionen in R 
================================================
```{r}
library(functional)
# linarr: n Werte einer Gerade a + b*x (im Intervall [0,1])
linarr <- function(n,a,b) a + b * seq(0, 1, length.out = n)

# mk_lin_ab erzeugt Closure (a und b gefangen, n bleibt frei)
mk_lin_ab <- function(a,b) Curry(linarr, a=a, b=b)

#Liste von 5 zufälligen linearen Funktionen (Non Standard Evaluation)
random_lins = replicate(5,mk_lin_ab(runif(1), runif(1, min=0.8, max=1.2)))

# Wrapper für dynamischen Funktionsaufruf mit einen Parameter
call_with_n <- function(n) Curry(do.call, args=list(n))

twenty_values_each <- sapply(random_lins, call_with_n(20))

```

(Vergleich: stan_lm)

Zufällige Funktionen in R (Plot)
================================================
```{r, out.width = '70%'}
matplot(twenty_values_each, type="l")

```


Multivariate Normalverteilung I
========================================================

-korrellierte Normalverteilungen

-der zweite Schritt ist mit dem ersten schwach korreliert

```{r, out.width = '70%'}
library(ggplot2)
steps <- data.frame(t(replicate(1000, cumsum(rnorm(100)))))
ggplot(data=steps, aes(x=X1, y=X2)) + geom_density_2d() + coord_fixed()
```

(Simulation 1000 Random Walks)


Multivariate Normalverteilung II
========================================================

-korrellierte Normalverteilungen

-Schritt 99 ist stark mit Schritt 98 korelliert

```{r, out.width = '70%'}
library(ggplot2)
ggplot(data=steps, aes(x=X98, y=X99)) + geom_point() + coord_fixed()

```

Multivariate Normalverteilung III
========================================================

- Konditionierung einer Dimension -> restliche Dimensionen bleiben normalverteilt 

  A-posteriori bleibt also Gaußprozess

- Summen bzw. Linearkombinationen bleiben normalverteilt

```{r, out.width = '60%'}
plot(density(rnorm(1000, mean=1) + 2*rnorm(1000, mean=2)))
```



Referenzen Gaußprozesse I
========================================================

- https://distill.pub/2019/visual-exploration-gaussian-processes/

  sehr coole interaktive Einführung
  
- Richard Turner: "Introduction to Gaussian Processes"

  https://www.youtube.com/watch?v=Jv25sg-IYHU

- http://www.gaussianprocess.org/
  
  Rasmussen, Williams: "Gaussian Processes for Machine Learning"

- Kevin Murphy "Machine Learning: a Probabilistic Perspective"
  
  Kapitel 14 und 15 


Referenzen Gaußprozesse II
========================================================
- David MacKay "Information Theory, Inference, and Learning Algorithms"
   
  Teil V: zeigt den Zusammenhang mit Neuronalen Netzen
  
- Stan User's Guide

- Gaußprozesse in der Geostatistik
  
  https://github.com/NCAR/fields/blob/master/fieldsVignette.pdf

- David Duvenaud:"The Kernel Cookbook"

  https://www.cs.toronto.edu/~duvenaud/cookbook/

- GPyTorch: https://gpytorch.readthedocs.io/en/latest/

  geht thematisch bis "Deep Kernel Learning"
  