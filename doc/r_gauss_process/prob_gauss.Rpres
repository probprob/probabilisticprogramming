Gaußprozesse und probabilistisches Programmieren in R
========================================================
author: Christoph Schmalhofer
date: 2019-09-17
autosize: true

Gaußprozesse sind überall
========================================================


```{r gpytorch, echo=FALSE, fig.cap=""}
knitr::include_graphics("gpytorch.png")
```

Wahrscheinlichkeitsverteilung von Funktionen

Bayes: Daten -> A-posteriori Gaußprozess


Anwendungsbeispiele
========================================================

- Bayes Optimization (Dateneffizienz)
- Zeitreihen
- Nachrichtentechnik
- Geostatistik

```{r surface, echo=FALSE, fig.cap="NCAR fields package"}
knitr::include_graphics("surface_spatialProcess_elevation.png")
```

(NCAR fields package)

Interpolation 
========================================================
- Bayes Posterior versus Konfidenzintervall
```{r, echo=F }
library(ggplot2)
n=10
x<-1:n
y<-x + rnorm(n)

```


```{r interpolation, out_with="90%"}
ggplot(data=data.frame(x,y), aes(x=x,y=y)) + geom_point() +  geom_smooth(method="loess", level=0.99, span=0.6)

```

Interpolation Splines
========================================================

- Splines 

  https://stats.stackexchange.com/questions/144634/splines-vs-gaussian-process-regression


```{r splines }
library(splines)
ggplot(data=data.frame(x,y), aes(x=x,y=y)) + geom_point() + geom_smooth(method="lm", formula = "y ~ splines::bs(x,3)")
```
  
Einfache Gaußprozesse
========================================================

- Weißes gaußsches Rauschen
- Lineares Modell 
- Random Walk
- Ornstein-Uhlenbeck


Weißes gaußsches Rauschen
========================================================
```{r rauschen}
plot(rnorm(100))
```

Lineares Modell
========================================================

```{r linear}
plot(1:100 + rnorm(100, sd=5))
```

Random Walk I
========================================================
```{r randomwalk}
plot(cumsum(rnorm(100)))
```

Random Walk II
========================================================

```{r randomwalk2}
matplot(replicate(5, cumsum(rnorm(100))), type="l")
```

Ornstein-Uhlenbeck
=====================================================
```{r ornstein}
#https://quant.stackexchange.com/questions/1260/r-code-for-ornstein-uhlenbeck-process
ornstein_uhlenbeck <- function(T, n, x0, nu, lambda, sigma){
dw  <- rnorm(n, 0, sqrt(T/n))
dt  <- T/n
x <- c(x0)
for (i in 2:(n+1)) {
x[i]  <-  x[i-1] + lambda*(nu-x[i-1])*dt + sigma*dw[i-1]
}
return(x);
}
```

x0: Startwert

nu: Attraktor 

lambda: Attraktionskraft

Ornstein-Uhlenbeck (Plot)
========================================================

```{r plot_ornstein}
plot(ornstein_uhlenbeck(T=2,n=1000,x0=5,nu=0,lambda=2,sigma=0.5))
```

- https://math.stackexchange.com/questions/2292050/how-is-the-ornstein-uhlenbeck-process-stationary-if-the-mean-and-variance-are-ti

Gaußprozess Definition
========================================================

- f ~ GP( mu(x), k(x1, x2) )
- jede Teilmenge ist multivariat normal verteilt
- Erwartungswert: mu(x) 
- Kovarianz: k(x1, x2)

- Kovarianz schränkt Art der Funktionen ein 


Multivariate Normalverteilung 
========================================================

- korrelierte Normalverteilungen

- Random Walk: der zweite Schritt ist mit dem ersten schwach korreliert

```{r multivariat}
steps <- data.frame(t(replicate(1000, cumsum(rnorm(100)))))
ggplot(data=steps, aes(x=X1, y=X2)) + geom_density_2d() + coord_fixed()
```

(Simulation 1000 Random Walks)


Multivariate Normalverteilung 99 Schritte 
========================================================

- korrelierte Normalverteilungen

-Schritt 99 ist stark mit Schritt 98 korreliert

```{r multivariat_99}
ggplot(data=steps, aes(x=X98, y=X99)) + geom_point() + coord_fixed()

```

Multivariate Normalverteilung Eigenschaften I
========================================================

- Die marginalen Verteilungen sind immer normal

- Konditionierung einer Dimension -> restliche Dimensionen bleiben normalverteilt 

  A-posteriori bleibt also Gaußprozess

Multivariate Normalverteilung Eigenschaften II
========================================================

- Summen bzw. Linearkombinationen bleiben normalverteilt

```{r gauss_linearkombination}
plot(density(rnorm(1000, mean=1) + 2*rnorm(1000, mean=2)))
```

Multivariate Normalverteilung Persp unkorreliert
========================================================

```{r persp1}
m = diag(2)
e2 <- function(x,y) exp(-rowSums((cbind(x,y) %*% solve(m) * cbind(x,y))))
x <- y<- seq(-4,4, length.out=100)
persp(x=x, y=y, z=outer(x, y, function(a,b) e2(a,b)))
```


Multivariate Normalverteilung Persp korreliert
========================================================

```{r persp2}
# plotly interaktiv -> Projekt gaussian
m=rbind(c(1.1,0.5), c(0.5,3.1))
persp(x=x, y=y, z=outer(x, y, function(a,b) e2(a,b)))
```


Kovarianzfunktionen der Beispiele
========================================================
- weißes Rauschen

  k(x_i, x_j) = kronecker(i, j)

- Random Walk

  k(x1, x2) = min(x1, x2 ) (Einstein 1905)

- Ornstein-Uhlenbeck

  k(x1, x2) = .....


Kovarianzfunktionen
========================================================

- squared exponential, gaussian, rbf

  k(x1,x2) = sigma^2 * exp(-(x1-x2)^2 / 2*length^2 )
  
  homogen, isotrop, sehr glatt, length bestimmt Reichweite
  
- periodisch

- linear

- Duvenaud: Kernel Cookbook

  Murphy: Kapitel 14

- Raum für Kreativität

Gaußprozess Inferenz
========================================================

- multivariate Gaußverteilung ist sehr gutartig

-  Formeln für Posterior mu und Kovarianz (keine wilden Integrale)
  
- Kovarianz Formel mit Matrix Inverse -> kubische Komplexität

- Murphy: Kapitel 4

Inferenz mit GauPro 
========================================================


```{r gaupro}
library(GauPro)
n <- 12
x <- matrix(seq(0,1,length.out = n), ncol=1)
y <- sin(2*pi*x) + rnorm(n,0,1e-1)
gp <- GauPro::GauPro(X=x, Z=y)
curve(gp$pred(x));points(x,y)
```


GauPro Matern Kernel
========================================================

```{r matern}
kern <- Matern52$new(0)
gpk <- GauPro_kernel_model$new(matrix(x, ncol=1), y, kernel=kern, parallel=FALSE)
plot(gpk)
```


Zufallsvariablen -> zufällige Funktion
========================================================

- Wiener, Kolmogoroff in etwa 1930: 

 Wahrscheinlichkeitsverteilung über Funktionen
 
 (Kolmogorov consistency/extension theorem)
 
- Heunen, Kammar, Staton, Yang: 

  "But standard probability theory cannot support higher-order functions, that is, the category of measurable spaces is not cartesian closed"

Zufällige Funktionen in R 
================================================
```{r functional, out_width="70%"}
library(functional)
# linarr: n Werte einer Gerade a + b*x (im Intervall [0,1])
linarr <- function(n,a,b) a + b * seq(0, 1, length.out = n)

# mk_lin_ab erzeugt Closure (a und b gefangen, n bleibt frei)
mk_lin_ab <- function(a,b) Curry(linarr, a=a, b=b)

#Liste von 5 zufälligen linearen Funktionen (Non Standard Evaluation)
random_lins = replicate(5,mk_lin_ab(runif(1), runif(1, min=0.8, max=1.2)))

# Wrapper für dynamischen Funktionsaufruf mit einen Parameter
call_with_n <- function(n) Curry(do.call, args=list(n))

twenty_values_each <- sapply(random_lins, call_with_n(20))

```


Zufällige Funktionen in R (Plot)
================================================
```{r plot_randomfunctions}
matplot(twenty_values_each, type="l")

```


Stan Linear Model
========================================================

```{r stan_lm}


library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

library(rstanarm)
n = 100
x <- 1:n
y <-  12 + 4*x + rnorm(n, sd=13)
plot(y)

```

Stan Linear Model Fit 
========================================================
```{r stan_lm_fit}
fit <- stan_lm( y ~ x, prior = R2(location = 0.5, what="mean"), chains=1)
```

Stan Linear Model Summary 
========================================================

```{r summary_stan_lm}
summary(fit)
```

Stan Linear Model Plot
========================================================

```{r plot_stan_lm}
plot(fit)

```

Stan Gaußprozess Modell (Stan Examples Project)
========================================================

```{r show_gp-fit, echo=F}
writeLines(readLines("gp-fit.stan"))
```


Stan Gaußprozess Fit 
========================================================

```{r fit_gp, warning=FALSE}

stan_dat <- read_rdump('gp-fit.data.R')

fit_gp <- stan(file="gp-fit.stan", data=stan_dat,
               iter=200, chains=1)


```

Stan Gaußprozess Print
========================================================

```{r print_gp}

print(fit_gp, pars = c('rho','alpha','sigma'))

```

Stan Gaußprozess Plot
========================================================

```{r plot_gp}

plot(fit_gp)

```


Probabilistic Language Stuff: Stan, pqR
========================================================

- Stanc3 Type System (Monaden für Stan?)

- https://discourse.mc-stan.org/t/stanc3-type-system-overhaul/10988


  Sean Talts. "@enetsee has brought up the idea of refactoring this such that distributions are a first class concept in a new type system ..."


- The fundamental abstractions underlying BUGS and Stan as probabilistic programming languages

  https://statmodeling.stat.columbia.edu/2017/09/07/fundamental-abstractions-underlying-bugs-stan-probabilistic-programming-languages/


- Radford Neal: An R Language Extension for Automatic Differentiation

  https://radfordneal.wordpress.com/2019/07/06/automatic-differentiation-in-pqr/

Referenzen 
========================================================


- David MacKay: Introduction to Gaussian Processes

  https://www.ics.uci.edu/~welling/teaching/KernelsICS273B/gpB.pdf
  
- Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver

  https://distill.pub/2019/visual-exploration-gaussian-processes/

  sehr coole interaktive Einführung

- Functional Neural Process

- https://arxiv.org/abs/1906.08324http://videolectures.net/DLRLsummerschool2018_wilson_bayesian_neural_nets/

Referenzen Gaußprozesse I
========================================================

  
- Andreas Damianou: Gaussian process lecture 

  https://nbviewer.jupyter.org/github/adamian/adamian.github.io/blob/master/talks/Brown2016.ipynb
  
- Richard Turner: "Introduction to Gaussian Processes"

  https://www.youtube.com/watch?v=Jv25sg-IYHU

- scikit-learn: GP zur Klassifizierung - Vergleich mit anderen Methoden

  https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html

- scikit-learn: Kernel Beispiele

  https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_prior_posterior.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-prior-posterior-py


Referenzen Gaußprozesse II
========================================================

- http://www.gaussianprocess.org/
  
  Rasmussen, Williams: "Gaussian Processes for Machine Learning"

- Kevin Murphy "Machine Learning: a Probabilistic Perspective"
  
  Kapitel 4, 14 und 15 

- David MacKay "Information Theory, Inference, and Learning Algorithms"
   
  Teil V: Neuronalen Netze -> Gaußprozess
  

Referenzen Gaußprozesse III
========================================================

- Michael Betancourt: wie robust sind Gaußprozess Regressionen

  https://betanalpha.github.io/assets/case_studies/gp_part1/part1.html
  
- Stan User's Guide

- Gaußprozesse in der Geostatistik
  
  https://github.com/NCAR/fields/blob/master/fieldsVignette.pdf

- David Duvenaud:"The Kernel Cookbook"

  https://www.cs.toronto.edu/~duvenaud/cookbook/

- GPyTorch: https://gpytorch.readthedocs.io/en/latest/


- PyMC3

  https://docs.pymc.io/Gaussian_Processes.html
  